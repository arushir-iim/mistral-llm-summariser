{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cc8e061",
   "metadata": {},
   "source": [
    "Full Mistral Summarization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e9e9ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "from typing import List, Optional\n",
    "\n",
    "import pdfplumber\n",
    "import tiktoken\n",
    "import ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130b1fd6",
   "metadata": {},
   "source": [
    "Keyword Extraction Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed02ed12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def extract_top_keywords(\n",
    "    text: str,\n",
    "    top_k: int = 10\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract top keywords using frequency-based scoring.\n",
    "    Designed for legal judgments.\n",
    "    \"\"\"\n",
    "\n",
    "    # Basic stopwords (extend if needed)\n",
    "    stopwords = {\n",
    "        \"the\", \"and\", \"of\", \"which\", \"under\", \"to\", \"in\", \"for\", \"on\", \n",
    "        \"is\", \"are\", \"was\", \"were\", \"by\", \"that\", \"this\", \"it\",\n",
    "        \"be\", \"or\", \"an\", \"at\", \"from\", \"has\", \"have\", \"had\",\n",
    "        \"court\", \"judge\", \"judgment\", \"section\", \"case\", \"with\", \"as\",\n",
    "        \"petitioner\", \"respondent\", \"india\", \"law\", \"article\"\n",
    "    }\n",
    "\n",
    "    # Normalize\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "\n",
    "    words = [\n",
    "        w for w in text.split()\n",
    "        if len(w) > 3 and w not in stopwords\n",
    "    ]\n",
    "\n",
    "    freq = Counter(words)\n",
    "\n",
    "    return [word for word, _ in freq.most_common(top_k)]\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def extract_candidate_terms(text: str):\n",
    "    patterns = [\n",
    "        r'Article\\s+\\d+[A-Z]?',\n",
    "        r'Section\\s+\\d+[A-Z]?',\n",
    "        r'IPC\\s+\\d+',\n",
    "        r'Supreme Court',\n",
    "        r'Constitution',\n",
    "    ]\n",
    "\n",
    "    matches = []\n",
    "    for p in patterns:\n",
    "        matches.extend(re.findall(p, text))\n",
    "\n",
    "    # Capitalized multi-word phrases\n",
    "    caps = re.findall(r'\\b([A-Z][a-z]+(?:\\s+[A-Z][a-z]+){1,3})\\b', text)\n",
    "\n",
    "    return list(set(matches + caps))\n",
    "\n",
    "def extract_social_keywords_mistral(text: str, top_k: int = 10):\n",
    "    prompt = f\"\"\"\n",
    "You are extracting keywords from a Supreme Court judgment\n",
    "for SOCIAL LISTENING on Twitter, YouTube, and news platforms.\n",
    "\n",
    "Select up to {top_k} keywords or short phrases that:\n",
    "- People are likely to use when discussing this judgment online\n",
    "- Include case names, articles, sections, institutions, and issues\n",
    "- Are short, searchable, and commonly used in public discourse\n",
    "\n",
    "Avoid legal jargon and long sentences.\n",
    "\n",
    "TEXT:\n",
    "{text[:4000]}\n",
    "\n",
    "Return only a comma-separated list of keywords.\n",
    "\"\"\"\n",
    "\n",
    "    response = ollama.chat(\n",
    "        model=\"mistral\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        options={\"temperature\": 0.1}\n",
    "    )\n",
    "\n",
    "    return [k.strip() for k in response[\"message\"][\"content\"].split(\",\")]\n",
    "\n",
    "def get_social_listening_keywords(text: str):\n",
    "    rule_based = extract_candidate_terms(text)\n",
    "    llm_based = extract_social_keywords_mistral(text)\n",
    "\n",
    "    combined = set(rule_based + llm_based)\n",
    "\n",
    "    # Light cleanup\n",
    "    cleaned = [\n",
    "        k.replace(\"  \", \" \").strip()\n",
    "        for k in combined\n",
    "        if len(k.split()) <= 5\n",
    "    ]\n",
    "\n",
    "    return cleaned[:20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e9811dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticPDFExtractor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        min_line_length: int = 40,\n",
    "        min_sentence_words: int = 12,\n",
    "        stop_sections: Optional[List[str]] = None\n",
    "    ):\n",
    "        self.min_line_length = min_line_length\n",
    "        self.min_sentence_words = min_sentence_words\n",
    "        self.stop_sections = stop_sections or []\n",
    "\n",
    "    # ---------- Core extraction ----------\n",
    "    def extract_raw_text(self, pdf_path: str) -> str:\n",
    "        pages = []\n",
    "        with pdfplumber.open(pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    pages.append(text)\n",
    "        return \"\\n\".join(pages)\n",
    "\n",
    "    # ---------- Cleaning ----------\n",
    "    def clean_lines(self, text: str) -> str:\n",
    "        lines = text.split(\"\\n\")\n",
    "        cleaned = []\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "\n",
    "            if len(line) < self.min_line_length:\n",
    "                continue\n",
    "            if re.fullmatch(r\"\\d+\", line):\n",
    "                continue\n",
    "\n",
    "            line = re.sub(r\"\\s+\", \" \", line)\n",
    "            cleaned.append(line)\n",
    "\n",
    "        return \"\\n\".join(cleaned)\n",
    "\n",
    "    # ---------- Paragraph normalization ----------\n",
    "    def normalize_paragraphs(self, text: str) -> str:\n",
    "        text = re.sub(r'-\\n', '', text)\n",
    "        text = re.sub(r'\\n(?!\\n)', ' ', text)\n",
    "        text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
    "        return text.strip()\n",
    "\n",
    "    # ---------- Section pruning ----------\n",
    "    def remove_stop_sections(self, text: str) -> str:\n",
    "        lowered = text.lower()\n",
    "        for section in self.stop_sections:\n",
    "            idx = lowered.find(section)\n",
    "            if idx != -1:\n",
    "                text = text[:idx]\n",
    "                lowered = lowered[:idx]\n",
    "        return text\n",
    "\n",
    "    # ---------- Sentence compression ----------\n",
    "    def compress_sentences(self, text: str) -> str:\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "\n",
    "        kept = [\n",
    "            s for s in sentences\n",
    "            if len(s.split()) >= self.min_sentence_words\n",
    "            and not s.lower().startswith((\n",
    "                \"this paper\",\n",
    "                \"in this study\",\n",
    "                \"copyright\",\n",
    "                \"all rights reserved\"\n",
    "            ))\n",
    "        ]\n",
    "\n",
    "        return \" \".join(kept)\n",
    "\n",
    "    # ---------- Full pipeline ----------\n",
    "    def extract(self, pdf_path: str) -> str:\n",
    "        text = self.extract_raw_text(pdf_path)\n",
    "        text = self.clean_lines(text)\n",
    "        text = self.normalize_paragraphs(text)\n",
    "        text = self.remove_stop_sections(text)\n",
    "        text = self.compress_sentences(text)\n",
    "        return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a65ce18",
   "metadata": {},
   "source": [
    "Chunking Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa51dcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(\n",
    "    text: str,\n",
    "    chunk_tokens: int = 3000,\n",
    "    overlap_tokens: int = 200,\n",
    "    encoding_name: str = \"gpt2\"\n",
    ") -> List[str]:\n",
    "    enc = tiktoken.get_encoding(encoding_name)\n",
    "    tokens = enc.encode(text)\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "\n",
    "    while start < len(tokens):\n",
    "        end = start + chunk_tokens\n",
    "        chunk = enc.decode(tokens[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start = end - overlap_tokens\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027d29be",
   "metadata": {},
   "source": [
    "Mistral Summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "720ecdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunk_mistral(chunk: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "You are summarizing a portion of a legal judgment.\n",
    "\n",
    "Summarize the following text clearly and accurately.\n",
    "Preserve legal reasoning, key findings, and conclusions.\n",
    "Avoid speculation.\n",
    "\n",
    "TEXT:\n",
    "{chunk}\n",
    "\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    "    response = ollama.chat(\n",
    "        model=\"mistral\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        options={\n",
    "            \"temperature\": 0.2,\n",
    "            \"num_ctx\": 4096\n",
    "        }\n",
    "    )\n",
    "    return response[\"message\"][\"content\"].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aad2ee",
   "metadata": {},
   "source": [
    "5. Recursive Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "26e510e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_summaries(\n",
    "    summaries: List[str],\n",
    "    group_size: int = 6\n",
    ") -> str:\n",
    "    current = summaries\n",
    "\n",
    "    while len(current) > 1:\n",
    "        next_round = []\n",
    "\n",
    "        for i in range(0, len(current), group_size):\n",
    "            group = \"\\n\\n\".join(current[i:i + group_size])\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "Combine the following summaries into a single,\n",
    "clear and coherent legal summary.\n",
    "\n",
    "SUMMARIES:\n",
    "{group}\n",
    "\n",
    "FINAL SUMMARY:\n",
    "\"\"\"\n",
    "            response = ollama.chat(\n",
    "                model=\"mistral\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                options={\"temperature\": 0.2}\n",
    "            )\n",
    "            next_round.append(response[\"message\"][\"content\"].strip())\n",
    "\n",
    "        current = next_round\n",
    "        print(\"Reduction step complete. Remaining summaries:\", len(current))\n",
    "\n",
    "    return current[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5a1a47",
   "metadata": {},
   "source": [
    "6. File-Naming Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df443edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_output_name(pdf_path: str, suffix: str, ext: str = \"txt\") -> str:\n",
    "    base = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "    return f\"{base}_{suffix}.{ext}\"\n",
    "\n",
    "def get_judgement_name(pdf_path: str) -> str:\n",
    "    return os.path.splitext(os.path.basename(pdf_path))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0067db2b",
   "metadata": {},
   "source": [
    "7. Main Pipeline (Multiple PDFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b177d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "csv_output_path = \"judgement_keywords_social_listening.csv\"\n",
    "\n",
    "with open(csv_output_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\n",
    "        \"Judgement\",\n",
    "        \"Keywords Mistral\",\n",
    "        \"Summary Mistral\"\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5ab40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing: C:\\Users\\Arushi\\Documents\\Python\\IIMA\\PDFs\\2GScam_judgement.pdf ===\n",
      "\n",
      "=== Processing: C:\\Users\\Arushi\\Documents\\Python\\IIMA\\PDFs\\Aadhaar_right_to_privacy_judgement.pdf ===\n",
      "\n",
      "=== Processing: C:\\Users\\Arushi\\Documents\\Python\\IIMA\\PDFs\\article_370_judgement.pdf ===\n",
      "\n",
      "=== Processing: C:\\Users\\Arushi\\Documents\\Python\\IIMA\\PDFs\\Assam_CAA_judgement.pdf ===\n",
      "\n",
      "=== Processing: C:\\Users\\Arushi\\Documents\\Python\\IIMA\\PDFs\\Bhima_Koregaon_judgement.pdf ===\n",
      "\n",
      "=== Processing: C:\\Users\\Arushi\\Documents\\Python\\IIMA\\PDFs\\Covid_19_judgement.pdf ===\n",
      "\n",
      "=== Processing: C:\\Users\\Arushi\\Documents\\Python\\IIMA\\PDFs\\Hijab-Ban-Judgment.pdf ===\n",
      "\n",
      "=== Processing: C:\\Users\\Arushi\\Documents\\Python\\IIMA\\PDFs\\Ram_janmabhoomi_judgement.pdf ===\n",
      "\n",
      "=== Processing: C:\\Users\\Arushi\\Documents\\Python\\IIMA\\PDFs\\sabrimala_judgement.pdf ===\n",
      "\n",
      "=== Processing: C:\\Users\\Arushi\\Documents\\Python\\IIMA\\PDFs\\Section377_judgement.pdf ===\n",
      "\n",
      "=== Processing: C:\\Users\\Arushi\\Documents\\Python\\IIMA\\PDFs\\Stray_dogs_judgement.pdf ===\n",
      "\n",
      "=== Processing: C:\\Users\\Arushi\\Documents\\Python\\IIMA\\PDFs\\triple_talak_judgement.pdf ===\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    extractor = SemanticPDFExtractor(\n",
    "        min_line_length=50,\n",
    "        min_sentence_words=15,\n",
    "        stop_sections=[\n",
    "            \"table of contents\",\n",
    "            \"disclaimer\",\n",
    "            \"references\",\n",
    "            \"appendix\",\n",
    "            \"copyright\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    PDF_DIR = Path(r\"C:\\Users\\Arushi\\Documents\\Python\\IIMA\\PDFs\")  # folder containing your PDFs\n",
    "\n",
    "    pdf_paths = sorted(PDF_DIR.glob(\"*.pdf\"))\n",
    "    pdf_paths = [str(p) for p in pdf_paths]  # convert to strings if needed\n",
    "\n",
    "    for pdf_path in pdf_paths:\n",
    "        print(f\"\\n=== Processing: {pdf_path} ===\")\n",
    "\n",
    "        # 1. Extract + compress\n",
    "        compressed_text = extractor.extract(pdf_path)\n",
    "\n",
    "        compressed_file = make_output_name(pdf_path, \"compressed\")\n",
    "        with open(compressed_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(compressed_text)\n",
    "\n",
    "        #2. Chunk\n",
    "        chunks = chunk_text(compressed_text)\n",
    "        print(\"Total chunks:\", len(chunks))\n",
    "\n",
    "        # 3. Chunk summaries\n",
    "        chunk_summaries = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"Summarizing chunk {i+1}/{len(chunks)}...\")\n",
    "            summary = summarize_chunk_mistral(chunk)\n",
    "            chunk_summaries.append(summary)\n",
    "\n",
    "        chunk_summary_file = make_output_name(pdf_path, \"chunk_summaries\")\n",
    "\n",
    "        # 4. Final reduction\n",
    "        final_summary = reduce_summaries(chunk_summaries)\n",
    "\n",
    "        #5. Extract keywords\n",
    "        keywords = get_social_listening_keywords(compressed_text)\n",
    "\n",
    "        with open(make_output_name(pdf_path, \"keywords\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(\"\\n\".join(keywords))\n",
    "\n",
    "        judgement_name = get_judgement_name(pdf_path)\n",
    "\n",
    "        with open(csv_output_path, mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\n",
    "                judgement_name,\n",
    "                \", \".join(keywords),   # keywords in one cell\n",
    "                final_summary          # full Mistral summary in one cell\n",
    "            ])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d0f594",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
